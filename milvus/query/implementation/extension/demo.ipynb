{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0c1f14-06e6-4310-9513-e46d5fb2e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pub_extension_pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c984c74f-040a-4e61-93c0-2a33a44474d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters for predicate 'biolink:treats'\n",
      "  Timestamp: 2025-10-21T15:31:05.380874\n",
      "  Based on 375 examples\n",
      "Node availability: 1\n",
      "Node is availableâ€”continuing execution.\n",
      "Loaded 1 edges for predicate: biolink:treats\n",
      "\n",
      "======================================================================\n",
      "STARTING PUBMED EXTENSION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Step 1: Generating query embeddings for semantic search...\n",
      "  Using: concat + sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761246967.265477 5869023 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Final: Processed 1 edges, generated 1 query vectors\n",
      "Generated 1 query vectors\n",
      "\n",
      "Step 2: Performing semantic search...\n",
      "Loading collection: pubmed_sentence_00\n",
      "Finished and unloaded: pubmed_sentence_00\n",
      "Loading collection: pubmed_sentence_01\n",
      "Finished and unloaded: pubmed_sentence_01\n",
      "Loading collection: pubmed_sentence_02\n",
      "Finished and unloaded: pubmed_sentence_02\n",
      "Loading collection: pubmed_sentence_03\n",
      "Finished and unloaded: pubmed_sentence_03\n",
      "Loading collection: pubmed_sentence_04\n",
      "Finished and unloaded: pubmed_sentence_04\n",
      "Loading collection: pubmed_sentence_05\n",
      "Finished and unloaded: pubmed_sentence_05\n",
      "Loading collection: pubmed_sentence_06\n",
      "Finished and unloaded: pubmed_sentence_06\n",
      "Loading collection: pubmed_sentence_07\n",
      "Finished and unloaded: pubmed_sentence_07\n",
      "Loading collection: pubmed_sentence_08\n",
      "Finished and unloaded: pubmed_sentence_08\n",
      "Loading collection: pubmed_sentence_09\n",
      "Finished and unloaded: pubmed_sentence_09\n",
      "Semantic search execution time: 815.75s\n",
      "Search complete. Found contexts for 1 edges\n",
      "\n",
      "Saved results to: result/semantic_search/treats_semantic_search.json\n",
      "Saved results to: result/semantic_search/treats_semantic_search_counts.json\n",
      "Step 3: Classifying abstracts...\n",
      "  Using: ai + sentence-transformers/all-MiniLM-L6-v2\n",
      "  Final: Processed 1 edges, generated 1 query vectors\n",
      "\n",
      "  Collecting unique PMIDs...\n",
      "  Total unique PMIDs to retrieve: 507\n",
      "\n",
      "  Retrieving abstracts from PubMed...\n",
      "  Retrieving batch 1/6...\n",
      "  Retrieving batch 2/6...\n",
      "  Retrieving batch 3/6...\n",
      "  Retrieving batch 4/6...\n",
      "  Retrieving batch 5/6...\n",
      "  Retrieving batch 6/6...\n",
      "  Retrieved 507 abstracts\n",
      "\n",
      "  Classifying abstracts for each edge...\n",
      "\n",
      "  Classification complete:\n",
      "  Edges with supporting PMIDs: 1\n",
      "  Total supporting relationships: 102\n",
      "Classification complete\n",
      "\n",
      "Saved results to: result/classification/treats_classified_abstracts.json\n",
      "Caching 507 abstracts for validation stage...\n",
      "Saved results to: result/classification/treats_cached_abstracts.json\n",
      "Step 4: LLM validation with two-round pipeline...\n",
      "\n",
      "======================================================================\n",
      "STARTING LLM VALIDATION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "[Step 1] Preparing validation prompts...\n",
      "Generated 102 prompts\n",
      "\n",
      "[Step 2] Running Round 1 validation...\n",
      "Starting Round 1 with model: gpt-oss:20b\n",
      "  Processed 50/102 prompts\n",
      "  Processed 100/102 prompts\n",
      "\n",
      "Round 1 Summary:\n",
      "  Total: 102\n",
      "  Successful: 102\n",
      "  Failed: 0\n",
      "  Support distribution:\n",
      "    no: 82\n",
      "    yes: 13\n",
      "    maybe: 7\n",
      "\n",
      "Filtering results:\n",
      "  Supporting ('yes'/'maybe'): 20\n",
      "  Non-supporting ('no'): 82 (filtered out)\n",
      "\n",
      "[Step 3] Running Round 2 validation...\n",
      "Starting Round 2 with model: gpt-oss:120b\n",
      "Validating 20 'yes' and 'maybe' cases for quality assurance\n",
      "\n",
      "Round 2 Summary:\n",
      "  Total: 20\n",
      "  Successful: 20\n",
      "  Failed: 0\n",
      "  Support distribution:\n",
      "    yes: 13\n",
      "    maybe: 5\n",
      "    no: 2\n",
      "\n",
      "[Step 4] Merging results from both rounds...\n",
      "\n",
      "Merging results from both rounds...\n",
      "  Round 2 identified 2 additional 'no' cases\n",
      "\n",
      "Merged results:\n",
      "  Supporting results ('yes'/'maybe'): 18\n",
      "  Non-supporting results ('no'): 84\n",
      "\n",
      "Final supporting validation statistics:\n",
      "  yes: 13\n",
      "  maybe: 5\n",
      "\n",
      "[Step 5] Adding abstract sentences to supporting results...\n",
      "\n",
      "Adding abstract sentences to validation data...\n",
      "\n",
      "[Step 6] Mapping LLM sentences to abstract indices...\n",
      "\n",
      "=== Mapping LLM sentences to abstract indices ===\n",
      "\n",
      "Mapping Statistics:\n",
      "Successful: 18\n",
      "Failed: 0\n",
      "Success rate: 100.0%\n",
      "\n",
      "[Step 7] Fixing failed mappings...\n",
      "No failed mappings to fix!\n",
      "\n",
      "======================================================================\n",
      "VALIDATION PIPELINE COMPLETE\n",
      "======================================================================\n",
      "Validation complete\n",
      "\n",
      "Saved supporting results to: result/validation/treats_validation_results.parquet\n",
      "Saved results to: result/validation/treats_llm_validated.json\n",
      "Saved non-supporting results to: result/validation/treats_non_supporting_results.parquet\n",
      "Saved results to: result/validation/treats_non_supporting.json\n",
      "Saved results to: result/semantic_search/treats_pipeline_statistics.json\n",
      "======================================================================\n",
      "PIPELINE COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from response_parser import SimpleLLMResponseParser\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize clients\n",
    "llm_client = Client()\n",
    "response_parser = SimpleLLMResponseParser()  # Your response parser\n",
    "\n",
    "edges = pd.read_parquet('edges/treats_nopub.parquet')\n",
    "edges = edges.loc[[1]]\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = PubExtensionPipeline(\n",
    "    predicate='biolink:treats',\n",
    "    llm_client=llm_client,\n",
    "    response_parser=response_parser,\n",
    "    edges=edges\n",
    ")\n",
    "\n",
    "await pipeline.run(max_edges=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fa212-f3af-4ef6-bff4-5844ac95a194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QARAG",
   "language": "python",
   "name": "qarag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
