{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384aff9b-92d6-458c-a81a-9768a2edb2a8",
   "metadata": {},
   "source": [
    "This notebook demonstrates the edge validation workflow using LLMs (gpt-oss:20b and gpt-oss:120b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be74da3-6aa3-47ac-883a-f5c9ed911181",
   "metadata": {},
   "source": [
    "### Step 1: Prepare edges for validation\n",
    "\n",
    "As a quick example, this notebook uses a small subset of 'treat' edges from RTX-KG2 sourced from SemMedDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9743a165-3f8e-4911-ae4c-066a4adf6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "semmed_treat_edges_df = pd.read_parquet(\"data/arax_rtx_273_semmed_treat_edges.parquet\")\n",
    "semmed_treat_edges_small_df = semmed_treat_edges_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02352f0-502d-4911-9d4b-dfc4d67c2011",
   "metadata": {},
   "source": [
    "#### Step 1.1: Load additional node and predicate information to provide more context for edge validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d89112-72ae-43de-a67d-c5242bfc59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dict/rtx-kg2_id_info_dictionary.json\", 'r') as file:\n",
    "    note_dict = json.load(file)\n",
    "\n",
    "with open(\"dict/biolink_pred_info_dictionary.json\", 'r') as file:\n",
    "    predicate_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69803e70-e437-45e4-ac66-1083eaeaed8b",
   "metadata": {},
   "source": [
    "### Step 2: Prompt generation\n",
    "\n",
    "Generate prompts that include node details for the subject and object, as well as predicate details from the BioLink model.\n",
    "The LLM is asked to classify the relationship between an edge and the PubMed abstract listed as evidence or source for that edge:\n",
    "\n",
    "- 'yes': The abstract supports and provides evidence for the edge\n",
    "- 'no': The abstract is irrelevant or contradicts the edge\n",
    "- 'maybe': The relationship between the edge and abstract is unclear (allows the LLM to express uncertainty)\n",
    "\n",
    "For 'yes' responses, the LLM must provide exact sentences from the abstract as evidence. This requirement encourages deeper reasoning, which may reduce hallucination and agreement bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1a89f5-d4e1-4335-9766-e3e9c6e49bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(subj_info, obj_info, pred_info, abstract):\n",
    "    return f\"\"\"Please analyze whether the provided abstract supports the following edge. \n",
    "Carefully consider the subject, object, and predicate details. \n",
    "\n",
    "Edge: {subj_info['name']} --{pred}-> {obj_info['name']}\n",
    "Subject: {subj_info}\n",
    "Object: {obj_info}\n",
    "Predicate: {pred_info}\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\n",
    "Instructions:\n",
    "- Determine if the abstract provides evidence for this edge.\n",
    "- Use \"yes\" if the relation is explicitly supported.\n",
    "- Use \"no\" if the relation is not mentioned or contradicted.\n",
    "- Use \"maybe\" if the evidence is indirect, ambiguous, or suggestive.\n",
    "- If \"Support?\" is \"yes\", return one or more exact supporting sentences from the abstract.\n",
    "- If \"Support?\" is \"no\" or \"maybe\", return an empty list for \"Sentences\".\n",
    "\n",
    "Output Format: Return only a JSON object in the following structure:\n",
    "{{\n",
    "  \"Support?\": \"yes\" | \"no\" | \"maybe\",\n",
    "  \"Sentences\": [\"...\"]  // one or more if yes, [] if no/maybe\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97670d-95a5-49b4-8a89-4482aca929a1",
   "metadata": {},
   "source": [
    "`get_publication_info` is a custom method from https://github.com/np0625/trapi-summarizer/blob/main/pubmed_client.py that enables frequent requests to the PubMed database without requiring an email address or prior authorization.\n",
    "\n",
    "The following procedure generates validation prompts. Note that edges are skipped if they cannot be enriched with additional details (node and predicate information) or if the abstract is unavailable, since edges with insufficient information cannot be validated fairly or precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a199cb2-7846-4bda-ba98-acc255ee949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pubmed_client import get_publication_info\n",
    "\n",
    "prompts = []\n",
    "for i, row in semmed_treat_edges_small_df.iterrows():    \n",
    "    subj = row['subject']\n",
    "    subj_info = note_dict.get(subj)\n",
    "    if subj_info:\n",
    "        obj = row['object']\n",
    "        obj_info = note_dict.get(obj)\n",
    "        if obj_info:\n",
    "            pred = row['predicate']\n",
    "            pred_info = predicate_dict.get(pred)\n",
    "            if pred_info:\n",
    "                pmids = row['publications']\n",
    "                pmids = [id for id in pmids if id.startswith('PMID:')]\n",
    "                if pmids:\n",
    "                    abstracts_info = await get_publication_info(pmids, 'placeholder')\n",
    "                    if abstracts_info['_meta']['n_results'] > 0: \n",
    "                        abstracts = abstracts_info['results']\n",
    "                        for pmid in pmids:\n",
    "                            abstract = abstracts.get(pmid, {})\n",
    "                            abstract = abstract.get('abstract')\n",
    "                            if abstract:\n",
    "                                prompt = generate_prompt(subj_info, obj_info, pred_info, abstract)\n",
    "                                prompts.append({'index': i, 'pmid': pmid, 'prompt': prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e83f9c9a-a71e-4ddf-a23b-548554a9bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prompt for future use, review, etc.\n",
    "\n",
    "# with open(\"intermedia_data/10_edge_validate_prompts.json\", \"w\") as file:\n",
    "#     json.dump(prompts, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9def3-34e4-4b56-a0ca-3cbfd355506f",
   "metadata": {},
   "source": [
    "### Step 3: Begin LLM validation (Round 1) and save responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d632f15-6e00-46d4-aae3-9f6f936106ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 10/116\n",
      "progress: 20/116\n",
      "progress: 30/116\n",
      "progress: 40/116\n",
      "progress: 50/116\n",
      "progress: 60/116\n",
      "progress: 70/116\n",
      "progress: 80/116\n",
      "progress: 90/116\n",
      "progress: 100/116\n",
      "progress: 110/116\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "responses_round1 = []\n",
    "client = Client()\n",
    "count = 0\n",
    "for prompt_info in prompts:\n",
    "    index = prompt_info['index']\n",
    "    pmid = prompt_info['pmid']\n",
    "    prompt = prompt_info['prompt']\n",
    "    messages = [\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': prompt,\n",
    "    },\n",
    "    ]\n",
    "    \n",
    "    response = client.chat(\n",
    "      model='gpt-oss:20b',\n",
    "      messages=messages,\n",
    "      options={'num_ctx': 8192},  # 8192 is the recommended lower limit for the context window\n",
    "    )\n",
    "    \n",
    "    responses_round1.append({'index': index, 'pmid': pmid, 'response': response['message']['content']})\n",
    "    count += 1\n",
    "\n",
    "\n",
    "\n",
    "    if count % 10 == 0:\n",
    "        with open(\"intermedia_data/10_edge_LLM_validate_responses_round1.json\", \"w\") as file:\n",
    "          json.dump(responses_round1, file, indent=4)\n",
    "        \n",
    "        print(f\"progress: {count}/{len(prompts)}\")\n",
    "\n",
    "with open(\"intermedia_data/10_edge_LLM_validate_responses_round1.json\", \"w\") as file:\n",
    "  json.dump(responses_round1, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9b411-e820-4eff-9ed7-0d8aaf492df3",
   "metadata": {},
   "source": [
    "#### Step 3.1: Parse LLM responses from Round 1, then save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20895d35-dae3-4500-b875-84ba20a3e8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 116\n",
      "==================================================\n",
      "RESPONSE PATTERNS:\n",
      "  plain_json: 116 (100.0%)\n",
      "\n",
      "==================================================\n",
      "SAMPLE RESPONSES BY PATTERN:\n",
      "\n",
      "PLAIN_JSON:\n",
      "  Sample 1 (record 0):\n",
      "    '{\\n  \"Support?\": \"yes\",\\n  \"Sentences\": [\\n    \"Norfloxacin is as effective as spectinomycin in gonorrhoea due to penicillin-resistant N. gonorrhoeae, and cures bacterial gastroenteritis caused by several gastrointestinal pathogens.\"\\n  ]\\n}'\n",
      "  Sample 2 (record 1):\n",
      "    '{\\n  \"Support?\": \"no\",\\n  \"Sentences\": []\\n}'\n",
      "  Sample 3 (record 2):\n",
      "    '{\"Support?\":\"no\",\"Sentences\":[] }'\n"
     ]
    }
   ],
   "source": [
    "from response_parser import *\n",
    "\n",
    "patterns, samples, errors = analyze_json_responses(responses_round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6968478a-f2e3-4fb0-9eaa-fc5a7652fcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Summary:\n",
      "  Total records: 116\n",
      "  Successful: 116\n",
      "  Failed: 0\n",
      "  Success rate: 100.0%\n",
      "error index: []\n"
     ]
    }
   ],
   "source": [
    "parser = LLMResponseParser()\n",
    "results_round1 = parser.parse_file(responses_round1)\n",
    "parser.save_results(results_round1, \"intermedia_data/parsed_10_edge_LLM_validate_responses_round1.json\")\n",
    "parser.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba7ab8-032a-4dc2-898a-5a87048e35ed",
   "metadata": {},
   "source": [
    "#### Step 3.2: Count categorical responses from Round 1 and extract 'yes' and 'maybe' responses for Round 2 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3560488-e484-4f50-b3f2-2ea787f939e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'no': 72, 'yes': 29, 'maybe': 15})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_values = [item['extracted_data']['Support?'] for item in results_round1]\n",
    "support_counter = Counter(support_values)\n",
    "support_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ead812dd-b85a-49d9-8f7a-edf78f0730fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_no_results = [x for x in results_round1 if x['extracted_data']['Support?'] != 'no']\n",
    "len(not_no_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dffcf0b3-3e24-467b-94a3-61a8923897cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_no_results_index_pmid = [(x['index'], x['pmid']) for x in not_no_results]\n",
    "len(not_no_results_index_pmid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2674fc-2b6c-4585-988a-e0e1fec81f86",
   "metadata": {},
   "source": [
    "### Step 4: Begin LLM validation (Round 2) and save responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "362b0240-ac1d-4e7a-9533-1e27c65becdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_round2 = [] \n",
    "count = 0\n",
    "client = Client()\n",
    "set_not_no_results_index_pmid = set(not_no_results_index_pmid)\n",
    "\n",
    "for prompt_info in prompts:\n",
    "    index = prompt_info['index']\n",
    "    pmid = prompt_info['pmid']\n",
    "    if (index, pmid) in set_not_no_results_index_pmid:\n",
    "        prompt = prompt_info['prompt']\n",
    "        messages = [\n",
    "          {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "          },\n",
    "        ]\n",
    "    \n",
    "        response = client.chat(\n",
    "            model='gpt-oss:120b',\n",
    "            messages=messages,\n",
    "            options={'num_ctx': 8192},  # 8192 is the recommended lower limit for the context window\n",
    "          )\n",
    "\n",
    "        responses_round2.append({'index': index, 'pmid': pmid, 'response': response['message']['content']})\n",
    "        count += 1\n",
    "        \n",
    "        if count % 10 == 0:    \n",
    "            with open(\"intermedia_data/10_edge_LLM_validate_responses_round2.json\", \"w\") as file:\n",
    "                json.dump(responses_round2, file, indent=4)\n",
    "    \n",
    "            print(f\"progress: {count}/{len(set_to_process)}\")\n",
    "\n",
    "with open(\"intermedia_data/10_edge_LLM_validate_responses_round2.json\", \"w\") as file:\n",
    "    json.dump(responses_round2, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4d025-a155-4ea8-bd0c-80a2f62389b2",
   "metadata": {},
   "source": [
    "#### Step 4.1: Parse LLM responses from Round 2, then save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7b23481-004a-4915-880c-89a534529721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 44\n",
      "==================================================\n",
      "RESPONSE PATTERNS:\n",
      "  plain_json: 44 (100.0%)\n",
      "\n",
      "==================================================\n",
      "SAMPLE RESPONSES BY PATTERN:\n",
      "\n",
      "PLAIN_JSON:\n",
      "  Sample 1 (record 0):\n",
      "    '{\\n  \"Support?\": \"yes\",\\n  \"Sentences\": [\\n    \"Nor... is as effective as spectinomycin in gonorrhoea due to penicillin-resistant N. gonorrhoeae, and cures bacterial gastroenteritis caused by several gastrointestinal pathogens.\"\\n  ]\\n}'\n",
      "  Sample 2 (record 1):\n",
      "    '{\\n  \"Support?\": \"maybe\",\\n  \"Sentences\": []\\n}'\n",
      "  Sample 3 (record 2):\n",
      "    '{\\n  \"Support?\": \"maybe\",\\n  \"Sentences\": []\\n}'\n"
     ]
    }
   ],
   "source": [
    "patterns, samples, errors = analyze_json_responses(responses_round2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4720580-c6d2-4a10-b5f5-e3c8377aff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Summary:\n",
      "  Total records: 44\n",
      "  Successful: 44\n",
      "  Failed: 0\n",
      "  Success rate: 100.0%\n",
      "error index: []\n"
     ]
    }
   ],
   "source": [
    "parser = LLMResponseParser()\n",
    "results_round2 = parser.parse_file(responses_round2)\n",
    "parser.save_results(results_round2, \"intermedia_data/parsed_10_edge_LLM_validate_responses_round2.json\")\n",
    "parser.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b08bc1-f684-42a2-a07a-12c04203acdb",
   "metadata": {},
   "source": [
    "Note: The following helper functions can be used to manually correct LLM responses. It's normal for LLMs to occasionally generate improperly formatted JSON responses, especially when processing large batches of requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d8eb4-3d08-49a1-be8f-99a523b87a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save failed records for manual fixing\n",
    "# parser.save_failed_records('failed_records_to_fix.json')\n",
    "\n",
    "# # After manually fixing failed_records_to_fix.json, load and reprocess:\n",
    "# with open('failed_records_to_fix.json', 'r') as f:\n",
    "#     fixed_data = json.load(f)\n",
    "\n",
    "# # Parse the fixed records\n",
    "# parser2 = LLMResponseParser()\n",
    "# fixed_results = parser2.parse_file(fixed_data)\n",
    "# parser2.print_summary()\n",
    "\n",
    "# # Combine with original results if needed\n",
    "# all_results = results + fixed_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QARAG",
   "language": "python",
   "name": "qarag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
